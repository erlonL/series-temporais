{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erlonL/series-temporais/blob/main/ST_Projeto2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wx5yIpzsIYNn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.43-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pandas_market_calendars\n",
            "  Downloading pandas_market_calendars-4.4.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pandas_datareader\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /home/erlo/.local/lib/python3.12/site-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/lib/python3/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/lib/python3/dist-packages (from yfinance) (2.31.0)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/lib/python3/dist-packages (from yfinance) (5.2.1)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/lib/python3/dist-packages (from yfinance) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/lib/python3/dist-packages (from yfinance) (2024.1)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Downloading frozendict-2.4.4-py312-none-any.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /usr/lib/python3/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/lib/python3/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from snscrape) (3.13.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from pandas_market_calendars) (2.8.2)\n",
            "Collecting exchange-calendars>=3.3 (from pandas_market_calendars)\n",
            "  Downloading exchange_calendars-4.5.6-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Collecting pyluach (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting toolz (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: tzdata in /home/erlo/.local/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2024.1)\n",
            "Collecting korean-lunar-calendar (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->snscrape)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading yfinance-0.2.43-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_market_calendars-4.4.1-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.5.6-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozendict-2.4.4-py312-none-any.whl (16 kB)\n",
            "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: peewee\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.6-py3-none-any.whl size=138891 sha256=9c21c6f97cfccd020853f95cb1f53bcf493ae17106c0a497f80c673ce4eec20a\n",
            "  Stored in directory: /home/erlo/.cache/pip/wheels/a6/5e/0f/8319805c4115320e0d3e8fb5799b114a2e4c4a3d6c7e523b06\n",
            "Successfully built peewee\n",
            "Installing collected packages: peewee, multitasking, korean-lunar-calendar, toolz, PySocks, pyluach, frozendict, yfinance, pandas_datareader, exchange-calendars, snscrape, pandas_market_calendars\n",
            "Successfully installed PySocks-1.7.1 exchange-calendars-4.5.6 frozendict-2.4.4 korean-lunar-calendar-0.3.1 multitasking-0.0.11 pandas_datareader-0.10.0 pandas_market_calendars-4.4.1 peewee-3.17.6 pyluach-2.2.0 snscrape-0.7.0.20230622 toolz-0.12.1 yfinance-0.2.43\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance snscrape pandas_market_calendars pandas_datareader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "58s0XzfaIW7t"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "#import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas_market_calendars as mcal\n",
        "import pandas_datareader.data as web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import snscrape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/erlo/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-09-20 09:41:23.434615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-20 09:41:23.442938: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-20 09:41:23.530854: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-20 09:41:23.608662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-20 09:41:23.692980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-20 09:41:23.717673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-20 09:41:23.858772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-20 09:41:25.021688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model = pipeline(\n",
        "    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
        "    return_all_scores=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGVYsNgkPlVM"
      },
      "source": [
        "## ETAPA 1: Coleta e Tratamento de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_DZA83VdYZ"
      },
      "source": [
        "S&P 500, abreviação de Standard & Poor's 500, ou simplesmente S&P, trata-se de um índice composto por quinhentos ativos cotados nas bolsas de NYSE ou NASDAQ, qualificados devido ao seu tamanho de mercado, sua liquidez e sua representação de grupo industrial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPPwmcj9H95x",
        "outputId": "ba8d06fd-9d0f-4eae-9d0e-8fd75daa7e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               Open      High      Low     Close    Volume  Dividends  \\\n",
            "2022-07-01  98542.0   99340.0  97231.0   98954.0  11609800        0.0   \n",
            "2022-07-04  98952.0   99353.0  98264.0   98609.0   6279300        0.0   \n",
            "2022-07-05  98608.0   98608.0  96499.0   98295.0  13358800        0.0   \n",
            "2022-07-06  98294.0   99141.0  97423.0   98719.0  13348200        0.0   \n",
            "2022-07-07  98722.0  101420.0  98722.0  100730.0  12696300        0.0   \n",
            "\n",
            "            Stock Splits  \n",
            "2022-07-01           0.0  \n",
            "2022-07-04           0.0  \n",
            "2022-07-05           0.0  \n",
            "2022-07-06           0.0  \n",
            "2022-07-07           0.0  \n"
          ]
        }
      ],
      "source": [
        "# Defina o ticker para o índice S&P 500\n",
        "ticker = '^BVSP'\n",
        "\n",
        "# Crie um objeto Ticker\n",
        "index = yf.Ticker(ticker)\n",
        "\n",
        "# Defina o intervalo de datas\n",
        "start_date = '2022-07-01'\n",
        "end_date = '2023-03-30'\n",
        "\n",
        "# Obtenha os dados históricos\n",
        "data = index.history(start=start_date, end=end_date)\n",
        "\n",
        "# deixar apenas a data (y/m/d) na coluna date\n",
        "data.index = data.index.date\n",
        "\n",
        "# Exiba os primeiros registros para verificação\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRVJu4efJYPU"
      },
      "source": [
        "No artigo, foi filtrado os dados para os dias úteis do mercado. Faremos a seguir esse processamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PyARdDhxJlXE"
      },
      "outputs": [],
      "source": [
        "nyse = mcal.get_calendar('NYSE')\n",
        "\n",
        "dias_uteis = nyse.valid_days(start_date=start_date, end_date=end_date)\n",
        "\n",
        "# Pegar so a data\n",
        "dias_uteis = [dia.date() for dia in dias_uteis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNjSIenGLstt",
        "outputId": "d3d1ecd8-7ac4-4b50-fe59-dfa432678a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Open      High      Low     Close    Volume  Dividends  \\\n",
            "2022-07-01   98542.0   99340.0  97231.0   98954.0  11609800        0.0   \n",
            "2022-07-05   98608.0   98608.0  96499.0   98295.0  13358800        0.0   \n",
            "2022-07-06   98294.0   99141.0  97423.0   98719.0  13348200        0.0   \n",
            "2022-07-07   98722.0  101420.0  98722.0  100730.0  12696300        0.0   \n",
            "2022-07-08  100732.0  101577.0  99958.0  100289.0   9730400        0.0   \n",
            "\n",
            "            Stock Splits  \n",
            "2022-07-01           0.0  \n",
            "2022-07-05           0.0  \n",
            "2022-07-06           0.0  \n",
            "2022-07-07           0.0  \n",
            "2022-07-08           0.0  \n"
          ]
        }
      ],
      "source": [
        "sp500 = data.loc[data.index.intersection(dias_uteis)]\n",
        "\n",
        "print(sp500.head())\n",
        "\n",
        "sp500.to_csv(\"sp500.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "gej-46DiO8Al",
        "outputId": "0acec7cb-4f12-4ee9-c8f7-283a17065673"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>181.000000</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>1.810000e+02</td>\n",
              "      <td>181.0</td>\n",
              "      <td>181.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>108481.872928</td>\n",
              "      <td>109537.839779</td>\n",
              "      <td>107416.430939</td>\n",
              "      <td>108510.370166</td>\n",
              "      <td>1.378456e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5384.554239</td>\n",
              "      <td>5440.204289</td>\n",
              "      <td>5262.758184</td>\n",
              "      <td>5347.174473</td>\n",
              "      <td>3.542855e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>96119.000000</td>\n",
              "      <td>96971.000000</td>\n",
              "      <td>95267.000000</td>\n",
              "      <td>96121.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>104375.000000</td>\n",
              "      <td>105497.000000</td>\n",
              "      <td>103409.000000</td>\n",
              "      <td>104385.000000</td>\n",
              "      <td>1.170710e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>109283.000000</td>\n",
              "      <td>110362.000000</td>\n",
              "      <td>108378.000000</td>\n",
              "      <td>109442.000000</td>\n",
              "      <td>1.338910e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>112307.000000</td>\n",
              "      <td>113306.000000</td>\n",
              "      <td>111380.000000</td>\n",
              "      <td>112323.000000</td>\n",
              "      <td>1.548190e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>119922.000000</td>\n",
              "      <td>120752.000000</td>\n",
              "      <td>117144.000000</td>\n",
              "      <td>119929.000000</td>\n",
              "      <td>2.602930e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Open           High            Low          Close  \\\n",
              "count     181.000000     181.000000     181.000000     181.000000   \n",
              "mean   108481.872928  109537.839779  107416.430939  108510.370166   \n",
              "std      5384.554239    5440.204289    5262.758184    5347.174473   \n",
              "min     96119.000000   96971.000000   95267.000000   96121.000000   \n",
              "25%    104375.000000  105497.000000  103409.000000  104385.000000   \n",
              "50%    109283.000000  110362.000000  108378.000000  109442.000000   \n",
              "75%    112307.000000  113306.000000  111380.000000  112323.000000   \n",
              "max    119922.000000  120752.000000  117144.000000  119929.000000   \n",
              "\n",
              "             Volume  Dividends  Stock Splits  \n",
              "count  1.810000e+02      181.0         181.0  \n",
              "mean   1.378456e+07        0.0           0.0  \n",
              "std    3.542855e+06        0.0           0.0  \n",
              "min    0.000000e+00        0.0           0.0  \n",
              "25%    1.170710e+07        0.0           0.0  \n",
              "50%    1.338910e+07        0.0           0.0  \n",
              "75%    1.548190e+07        0.0           0.0  \n",
              "max    2.602930e+07        0.0           0.0  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp500.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wB7DC6XkQfiE"
      },
      "outputs": [],
      "source": [
        "query = \"S&P500\"\n",
        "min_likes = 0\n",
        "xlsx_name = \"tweets_sp500.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error retrieving https://api.pushshift.io/reddit/search/submission?q=S%26P500&limit=1000: non-200 status code\n",
            "4 requests to https://api.pushshift.io/reddit/search/submission?q=S%26P500&limit=1000 failed, giving up.\n",
            "Errors: non-200 status code, non-200 status code, non-200 status code, non-200 status code\n"
          ]
        },
        {
          "ename": "ScraperException",
          "evalue": "4 requests to https://api.pushshift.io/reddit/search/submission?q=S%26P500&limit=1000 failed, giving up.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# search query for reddit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m posts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, post \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(snscrape\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mreddit\u001b[38;5;241m.\u001b[39mRedditSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, post\u001b[38;5;241m.\u001b[39mtitle, post\u001b[38;5;241m.\u001b[39murl, post\u001b[38;5;241m.\u001b[39mlikes, post\u001b[38;5;241m.\u001b[39mnum_comments, post\u001b[38;5;241m.\u001b[39mcreated)\n\u001b[1;32m      7\u001b[0m     posts\u001b[38;5;241m.\u001b[39mappend([post\u001b[38;5;241m.\u001b[39mtitle, post\u001b[38;5;241m.\u001b[39murl, post\u001b[38;5;241m.\u001b[39mlikes, post\u001b[38;5;241m.\u001b[39mnum_comments, post\u001b[38;5;241m.\u001b[39mcreated])\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/reddit.py:219\u001b[0m, in \u001b[0;36m_RedditPushshiftSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_items\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 219\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_submissions_and_comments({\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_apiField: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name})\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/reddit.py:185\u001b[0m, in \u001b[0;36m_RedditPushshiftSearchScraper._iter_api_submissions_and_comments\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    182\u001b[0m \tcommentsIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(())\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m \ttipSubmission \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubmissionsIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m \t\u001b[38;5;66;03m# There are no submissions, just yield comments and return\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m commentsIter\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/reddit.py:143\u001b[0m, in \u001b[0;36m_RedditPushshiftScraper._iter_api\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    141\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (lowestIdSeen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_cmp_id(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], lowestIdSeen) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])): \u001b[38;5;66;03m# end of pagination\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \t\t\u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/reddit.py:94\u001b[0m, in \u001b[0;36m_RedditPushshiftScraper._get_api\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_api\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 94\u001b[0m \tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rate_limiting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     96\u001b[0m \t\t\u001b[38;5;28;01mraise\u001b[39;00m snscrape\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mScraperException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[1;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.pushshift.io/reddit/search/submission?q=S%26P500&limit=1000 failed, giving up."
          ]
        }
      ],
      "source": [
        "# search query for reddit\n",
        "\n",
        "posts = []\n",
        "\n",
        "for i, post in enumerate(snscrape.modules.reddit.RedditSearchScraper(query).get_items()):\n",
        "    print(i, post.title, post.url, post.likes, post.num_comments, post.created)\n",
        "    posts.append([post.title, post.url, post.likes, post.num_comments, post.created])\n",
        "\n",
        "df = pd.DataFrame(posts, columns=[\"title\", \"url\", \"likes\", \"num_comments\", \"created\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "iyZPPlI0QwV2",
        "outputId": "61aa3ccb-e686-4162-bfe4-0f851df10058"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error retrieving https://twitter.com/search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click: ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'twitter.com\\', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click (Caused by NameResolutionError(\"<snscrape.base._HTTPSConnection object at 0x7aed9a6e3280>: Failed to resolve \\'twitter.com\\' ([Errno -2] Name or service not known)\"))'))\n",
            "4 requests to https://twitter.com/search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click failed, giving up.\n",
            "Errors: ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'twitter.com\\', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click (Caused by NameResolutionError(\"<snscrape.base._HTTPSConnection object at 0x7aed9a6e2aa0>: Failed to resolve \\'twitter.com\\' ([Errno -2] Name or service not known)\"))')), ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'twitter.com\\', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click (Caused by NameResolutionError(\"<snscrape.base._HTTPSConnection object at 0x7aed9a6e3790>: Failed to resolve \\'twitter.com\\' ([Errno -2] Name or service not known)\"))')), ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'twitter.com\\', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click (Caused by NameResolutionError(\"<snscrape.base._HTTPSConnection object at 0x7aed9a6e0d90>: Failed to resolve \\'twitter.com\\' ([Errno -2] Name or service not known)\"))')), ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'twitter.com\\', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click (Caused by NameResolutionError(\"<snscrape.base._HTTPSConnection object at 0x7aed9a6e3280>: Failed to resolve \\'twitter.com\\' ([Errno -2] Name or service not known)\"))'))\n"
          ]
        },
        {
          "ename": "ScraperException",
          "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click failed, giving up.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min_retweets:0 min_faves:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_likes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m since:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m until:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(search_query)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Extract relevant information from the tweet\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     tweet_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     12\u001b[0m     tweets\u001b[38;5;241m.\u001b[39mappend(tweet_data)\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/twitter.py:883\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, apiType, params, instructionsPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 883\u001b[0m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_guest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[1;32m    885\u001b[0m \t\tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/modules/twitter.py:825\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._ensure_guest_token\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guestTokenManager\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving guest token\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 825\u001b[0m \tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_baseUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_guest_token_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m (match \u001b[38;5;241m:=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.cookie = decodeURIComponent\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt=(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+); Max-Age=10800; Domain=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.twitter\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.com; Path=/; Secure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)):\n\u001b[1;32m    827\u001b[0m \t\t_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound guest token in HTML\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Code/UFPB/series-temporais/series-temporais-env/lib/python3.10/site-packages/snscrape/base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[1;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://twitter.com/search?f=live&lang=en&q=S%26P500+min_retweets%3A0+min_faves%3A0+since%3A2022-07-01+until%3A2023-03-30&src=spelling_expansion_revert_click failed, giving up."
          ]
        }
      ],
      "source": [
        "tweets = []\n",
        "\n",
        "search_query = f'{query} min_retweets:0 min_faves:{min_likes} since:{start_date} until:{end_date}'\n",
        "\n",
        "for tweet in sntwitter.TwitterSearchScraper(search_query).get_items():\n",
        "    # Extract relevant information from the tweet\n",
        "    tweet_data = {\n",
        "        \"tweet\": tweet.content,\n",
        "        \"Date\": tweet.date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "\n",
        "    tweets.append(tweet_data)\n",
        "\n",
        "tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 2: Análise e Agregação de Sentimento através do DistilRoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNE3Ns8Mekj1PFucHxd5BL8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
